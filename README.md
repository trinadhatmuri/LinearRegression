## ğŸ“ Description

This project demonstrates how Linear Regression works under the hood by:

- Deriving and coding the normal equation  
- Implementing gradient descent manually  
- Adding regularization technique (Ridge / L2 Regularization) to improve model generalization
- Comparing against scikitâ€‘learnâ€™s `LinearRegression`  
- Evaluating model performance (MSE, plots)  
- Documenting everything in a Jupyter Notebook  

---

## ğŸš€ Features

- **Pureâ€‘Python Implementation** â€“ No hidden libraries, just NumPy & math.  
- **Vectorized Computation** â€“ Fast matrix operations with NumPy.  
- **Model Comparison** â€“ Sideâ€‘byâ€‘side evaluation with scikitâ€‘learn.
- **Regularization Support** - Ridge Regression implementation from scratch.  
- **Visualization** â€“ Matplotlib charts of predictions vs. actuals.  
- **Clear Documentation** â€“ Stepâ€‘byâ€‘step guide in Notebook format.

## Disclamer
- This repository reflects my personal learning journey.
- I followed Andrew Ngâ€™s course for core concepts.
- I used LLM-based chatbots to troubleshoot and clarify implementation detailsâ€”any errors or misunderstandings are mine alone.
- Feel free to use or adapt this code for your own studies, but itâ€™s not intended as production-grade software.
