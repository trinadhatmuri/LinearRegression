## 📝 Description

This project demonstrates how Linear Regression works under the hood by:

- Deriving and coding the normal equation  
- Implementing gradient descent manually  
- Adding regularization technique (Ridge / L2 Regularization) to improve model generalization
- Comparing against scikit‑learn’s `LinearRegression`  
- Evaluating model performance (MSE, plots)  
- Documenting everything in a Jupyter Notebook  

---

## 🚀 Features

- **Pure‑Python Implementation** – No hidden libraries, just NumPy & math.  
- **Vectorized Computation** – Fast matrix operations with NumPy.  
- **Model Comparison** – Side‑by‑side evaluation with scikit‑learn.
- **Regularization Support** - Ridge Regression implementation from scratch.  
- **Visualization** – Matplotlib charts of predictions vs. actuals.  
- **Clear Documentation** – Step‑by‑step guide in Notebook format.

## Disclamer
- This repository reflects my personal learning journey.
- I followed Andrew Ng’s course for core concepts.
- I used LLM-based chatbots to troubleshoot and clarify implementation details—any errors or misunderstandings are mine alone.
- Feel free to use or adapt this code for your own studies, but it’s not intended as production-grade software.
